{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f83a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcabb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "    interpreter = None\n",
    "    policy = None\n",
    "\n",
    "    _trace = []\n",
    "    \n",
    "    def __init__(self, interpreter, policy):\n",
    "        self.interpreter = interpreter\n",
    "        self.policy = policy\n",
    "        self._trace = []\n",
    "    \n",
    "    def step(self, h):\n",
    "        state = self.interpreter.state()\n",
    "        actions = state.actions()\n",
    "        if len(actions) != 0:\n",
    "            random.shuffle(actions)\n",
    "            action = self.policy.next_action(h, state, actions)\n",
    "            if action != None:\n",
    "                self.interpreter.do(action)\n",
    "                next_state = self.interpreter.state()\n",
    "                self.policy.update(h, state, action, next_state)\n",
    "                self._trace.append((state, action))\n",
    "    \n",
    "    def next_episode(self):\n",
    "        self.policy.update_episode(self._trace)\n",
    "        self._trace = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263dc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplorationPolicy:\n",
    "    horizon = 0\n",
    "    state_space = 0\n",
    "    episode = 0\n",
    "    actions = 0\n",
    "    \n",
    "    prob = 0\n",
    "    c = 1\n",
    "    eta = 0.0\n",
    "\n",
    "    epsilon = 0.0\n",
    "    rand = random.Random()\n",
    "\n",
    "    state = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, horizon, state_space, episode, actions, prob, c, epsilon=0.0):\n",
    "        self.horizon = horizon\n",
    "        self.state_space = state_space\n",
    "        self.episode = episode\n",
    "        self.actions = actions\n",
    "        self.prob = prob\n",
    "        self.c = c\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.eta = math.log10(self.horizon * self.state_space * self.episode * self.actions / float(self.prob))\n",
    "        self.state = ExplorationState(self.horizon, self.c, self.eta)\n",
    "    \n",
    "    def next_action(self, step, state, actions):\n",
    "        if self.rand.random() < self.epsilon:\n",
    "            return random.choice(actions)\n",
    "        return self.state.next_action(step, state, actions)\n",
    "    \n",
    "    def update(self, step, state, action, next_state):\n",
    "        self.state.update(step, state, action, next_state)\n",
    "    \n",
    "    def update_episode(self, trace):\n",
    "        return None\n",
    "    \n",
    "class ExplorationState:\n",
    "    horizon = 0\n",
    "    c = 0\n",
    "    eta = 0\n",
    "    \n",
    "    qa_map = {}\n",
    "    visits = {}\n",
    "    state_values = {}\n",
    "\n",
    "    _default_qvalue = 0\n",
    "    \n",
    "    def __init__(self, horizon, c, eta):\n",
    "        self.horizon = horizon\n",
    "        self.c = c\n",
    "        self.eta = eta\n",
    "        self.qa_map = {}\n",
    "        self.visits = {}\n",
    "        self.state_values = {}\n",
    "\n",
    "        self._default_qvalue = self.horizon \n",
    "        # + (2*self.c * math.sqrt(math.pow(self.horizon, 3)*(self.eta))) + 100\n",
    "    \n",
    "    \n",
    "    def next_action(self, step, state, actions):\n",
    "        \n",
    "        state_key = state.to_string()\n",
    "        if step not in self.qa_map:\n",
    "            self.qa_map[step] = {}\n",
    "        \n",
    "        if state_key not in self.qa_map[step]:\n",
    "            self.qa_map[step][state_key] = {}\n",
    "            \n",
    "        max_val = 0\n",
    "        max_action = \"\"\n",
    "        \n",
    "        for a in actions:\n",
    "            if a not in self.qa_map[step][state_key]:\n",
    "                self.qa_map[step][state_key][a] = self._default_qvalue\n",
    "        \n",
    "        for a in actions:\n",
    "            val = self.qa_map[step][state_key][a]\n",
    "            if val > max_val:\n",
    "                max_val = val\n",
    "                max_action = a\n",
    "        \n",
    "        return max_action\n",
    "    \n",
    "    def _init(self, step, state, action = None):\n",
    "        state_key = state.to_string()\n",
    "        if step not in self.qa_map:\n",
    "            self.qa_map[step] = {}\n",
    "            self.qa_map[step][state_key] = {}\n",
    "            if action is not None:\n",
    "                self.qa_map[step][state_key][action] = self._default_qvalue\n",
    "\n",
    "        if step not in self.visits:\n",
    "            self.visits[step] = {}\n",
    "            self.visits[step][state_key] = {}\n",
    "            if action is not None:\n",
    "                self.visits[step][state_key][action] = 0\n",
    "\n",
    "        if step not in self.state_values:\n",
    "            self.state_values[step] = {}\n",
    "        \n",
    "        if state_key not in self.qa_map[step]:\n",
    "            self.qa_map[step][state_key] = {}\n",
    "            if action is not None:\n",
    "                self.qa_map[step][state_key][action] = self._default_qvalue\n",
    "\n",
    "        if state_key not in self.visits[step]:\n",
    "            self.visits[step][state_key] = {}\n",
    "            if action is not None:\n",
    "                self.visits[step][state_key][action] = 0\n",
    "\n",
    "        if action is not None:\n",
    "            if action not in self.qa_map[step][state_key]:\n",
    "                self.qa_map[step][state_key][action] = self._default_qvalue\n",
    "            if action not in self.visits[step][state_key]:\n",
    "                self.visits[step][state_key][action] = 0\n",
    "    \n",
    "    def update(self, step, state, action, next_state):\n",
    "        self._init(step, state, action)\n",
    "        self._init(step+1, next_state)\n",
    "\n",
    "        state_key = state.to_string()\n",
    "        next_state_key = next_state.to_string()\n",
    "\n",
    "        t = self.visits[step][state_key][action] + 1\n",
    "        self.visits[step][state_key][action] = t\n",
    "\n",
    "        max_val = 0\n",
    "        for a in self.qa_map[step+1][next_state_key]:\n",
    "            action_val = self.qa_map[step+1][next_state_key][a]\n",
    "            if action_val > max_val:\n",
    "                max_val = action_val\n",
    "        if len(self.qa_map[step+1][next_state_key]) == 0:\n",
    "            max_val = self.horizon\n",
    "\n",
    "        next_state_val = max_val\n",
    "        if next_state_val > self.horizon:\n",
    "            next_state_val = self.horizon\n",
    "        \n",
    "        if step+1 == self.horizon:\n",
    "            next_state_val = 0\n",
    "        \n",
    "        self.state_values[step+1][next_state_key] = next_state_val\n",
    "        cur_qa_val = self.qa_map[step][state_key][action]\n",
    "        alpha_t = float(self.horizon + 1)/float(self.horizon + t)\n",
    "        b_t = self.c * math.sqrt(math.pow(self.horizon, 3)*(self.eta) / t)\n",
    "\n",
    "        new_qa_val = ((1 - alpha_t) * cur_qa_val) + (alpha_t * (next_state_val + 2 * b_t))\n",
    "        self.qa_map[step][state_key][action] = new_qa_val\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3457354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    def next_action(self, step, state, actions):        \n",
    "        i = random.randrange(len(actions))\n",
    "        return actions[i]\n",
    "    \n",
    "    def update(self, step, state, action, next_state):\n",
    "        return None\n",
    "    \n",
    "    def update_episode(self, trace):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df03a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeRewardPolicy:\n",
    "    alpha = 0\n",
    "    gamma = 0\n",
    "    qa_map = {}\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.qa_map = {}\n",
    "\n",
    "    def next_action(self, step, state, actions):\n",
    "        state_key = state.to_string()\n",
    "        if state_key not in self.qa_map:\n",
    "            self.qa_map[state_key] = {}\n",
    "\n",
    "        sum = 0\n",
    "        weights = []\n",
    "        for a in actions:\n",
    "            if a not in self.qa_map[state_key]:\n",
    "                self.qa_map[state_key][a] = 0\n",
    "            val = math.exp(self.qa_map[state_key][a])\n",
    "            sum += val\n",
    "            weights.append(val)\n",
    "        \n",
    "        for i in range(len(actions)):\n",
    "            weights[i] = weights[i] / float(sum)\n",
    "        \n",
    "        return np.random.choice(actions, p=weights)\n",
    "    \n",
    "    def update(self, step, state, action, next_state):\n",
    "        return None\n",
    "    \n",
    "    def update_episode(self, trace):\n",
    "        for (state, action) in trace:\n",
    "            state_key = state.to_string()\n",
    "            if state_key not in self.qa_map:\n",
    "                continue\n",
    "\n",
    "            if action not in self.qa_map[state_key]:\n",
    "                self.qa_map[state_key][action] = 0\n",
    "\n",
    "            cur_val = self.qa_map[state_key][action]\n",
    "\n",
    "            max = 0\n",
    "            for a in self.qa_map[state_key]:\n",
    "                if self.qa_map[state_key][a] > max:\n",
    "                    max = self.qa_map[state_key][a]\n",
    "\n",
    "            next_val = (1 - float(self.alpha)) * cur_val + float(self.alpha) * (-1 + float(self.gamma) * max)\n",
    "            self.qa_map[state_key][action] = next_val\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb70eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridState:\n",
    "    size = 0\n",
    "    pos = (0,0)\n",
    "    blocked_row = 0\n",
    "    \n",
    "    def __init__(self, pos, size, blocked_row):\n",
    "        self.pos = pos\n",
    "        self.size = size\n",
    "        self.blocked_row = blocked_row\n",
    "    \n",
    "    def actions(self):\n",
    "        (i,j) = self.pos\n",
    "        actions = []\n",
    "        if i > 0:\n",
    "            actions.append(\"up\")\n",
    "        if i < (self.size-1):\n",
    "            actions.append(\"down\")\n",
    "        if j > 0 and j != self.blocked_row:\n",
    "            actions.append(\"left\")\n",
    "        if j < (self.size-1):\n",
    "            actions.append(\"right\")\n",
    "        return actions\n",
    "    \n",
    "    def to_string(self):\n",
    "        (i,j) = self.pos\n",
    "        return f'({i},{j})'\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a5e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridInterpreter:\n",
    "    grid = []\n",
    "    cur_pos = (0,0)\n",
    "    size = 0\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.grid = [[0 for i in range(size)] for i in range(size)]\n",
    "        self.cur_pos = (0,0)\n",
    "        self.grid[0][0] = 1\n",
    "        self.size = size\n",
    "    \n",
    "    def state(self):\n",
    "        return GridState(self.cur_pos, self.size)\n",
    "    \n",
    "    def do(self, action):\n",
    "        (i, j) = self.cur_pos\n",
    "        if action == \"left\":\n",
    "            self.cur_pos = (i, j-1)\n",
    "        elif action == \"right\":\n",
    "            self.cur_pos = (i, j+1)\n",
    "        elif action == \"up\":\n",
    "            self.cur_pos = (i-1, j)\n",
    "        elif action == \"down\":\n",
    "            self.cur_pos = (i+1, j)\n",
    "        (i,j) = self.cur_pos\n",
    "        self.grid[i][j] = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = (0,0)\n",
    "\n",
    "    def covered_positions(self):\n",
    "        count = 0\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.grid[i][j] == 1:\n",
    "                    count = count +1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistortedGridInterpreter:\n",
    "    grid = []\n",
    "    cur_pos = (0,0)\n",
    "    size = 0\n",
    "    narrow_row = 0\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.grid = [[0 for i in range(size)] for i in range(size)]\n",
    "        self.cur_pos = (0,0)\n",
    "        self.grid[0][0] = 1\n",
    "        self.size = size\n",
    "        self.narrow_row = size/2\n",
    "    \n",
    "    def state(self):\n",
    "        return GridState(self.cur_pos, self.size)\n",
    "    \n",
    "    def do(self, action):\n",
    "        (i, j) = self.cur_pos\n",
    "        if action == \"left\":\n",
    "            self.cur_pos = (i, j-1)\n",
    "        elif action == \"right\":\n",
    "            self.cur_pos = (i, j+1)\n",
    "        elif action == \"up\":\n",
    "            self.cur_pos = (i-1, j)\n",
    "        elif action == \"down\":\n",
    "            self.cur_pos = (i+1, j)\n",
    "        (i,j) = self.cur_pos\n",
    "        self.grid[i][j] = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = (0,0)\n",
    "\n",
    "    def covered_positions(self):\n",
    "        count = 0\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.grid[i][j] == 1:\n",
    "                    count = count +1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc212bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(params):\n",
    "    policy = params[\"policy\"]\n",
    "    if policy == \"ucbzero\":\n",
    "        horizon = params[\"horizon\"]\n",
    "        state_space = params[\"state_space\"]\n",
    "        actions = params[\"actions\"]\n",
    "        episodes = params[\"episodes\"]\n",
    "\n",
    "        prob = params[\"prob\"]\n",
    "        c = params[\"c\"]\n",
    "        grid_size = params[\"grid_size\"]\n",
    "        epsilon = params[\"epsilon\"]\n",
    "        return ExplorationPolicy(horizon, state_space, episodes, actions, prob, c, epsilon)\n",
    "    elif policy == \"softmax\":\n",
    "        alpha = params[\"alpha\"]\n",
    "        gamma = params[\"gamma\"]\n",
    "        return NegativeRewardPolicy(alpha, gamma)\n",
    "    elif policy == \"random\":\n",
    "        return RandomPolicy()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_rl(params):\n",
    "    policy = get_policy(params)\n",
    "    horizon = params[\"horizon\"]\n",
    "    episodes = params[\"episodes\"]\n",
    "    grid_size = params[\"grid_size\"]\n",
    "    \n",
    "    grid_interpreter = GridInterpreter(grid_size)\n",
    "\n",
    "    rl_agent = RLAgent(grid_interpreter,policy)\n",
    "\n",
    "    for i in range (episodes):\n",
    "        for j in range(horizon):\n",
    "            rl_agent.step(j)\n",
    "        rl_agent.next_episode()\n",
    "        \n",
    "        grid_interpreter.reset()\n",
    "    \n",
    "    return grid_interpreter.covered_positions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4011430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coverage:  369.1\n",
      "Average coverage:  357.5\n",
      "Average coverage 1:  369.1\n",
      "Average coverage 2:  357.5\n"
     ]
    }
   ],
   "source": [
    "def average_coverage(params, runs):\n",
    "    sum = 0\n",
    "    for i in range(runs):\n",
    "        sum += run_rl(params)\n",
    "\n",
    "    avg = sum/runs\n",
    "    print(\"Average coverage: \",avg)\n",
    "    return avg\n",
    "\n",
    "def compare(policy1, policy2, runs):\n",
    "    coverage1 = average_coverage(policy1, runs)\n",
    "    coverage2 = average_coverage(policy2, runs)\n",
    "\n",
    "    print(\"Average coverage 1: \", coverage1)\n",
    "    print(\"Average coverage 2: \", coverage2)\n",
    "    \n",
    "# average_coverage({\"policy\": \"ucbzero\",\"horizon\": 100, \"state_space\": 400, \"actions\": 4, \"episodes\": 1000, \"prob\": 0.2, \"c\": 0.001, \"grid_size\": 20, \"epsilon\": 0.1, \"alpha\": 0.3, \"gamma\": 0.7}, 10)\n",
    "\n",
    "compare({\"policy\": \"random\", \"horizon\": 100, \"episodes\": 1000, \"grid_size\": 20}, {\"policy\": \"ucbzero\",\"horizon\": 100, \"state_space\": 400, \"actions\": 4, \"episodes\": 1000, \"prob\": 0.2, \"c\": 0.001, \"grid_size\": 20, \"epsilon\": 0.1, \"alpha\": 0.3, \"gamma\": 0.7}, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5bf59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
